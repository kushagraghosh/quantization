{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 0 Download the dataset"
      ],
      "metadata": {
        "id": "9c-6qY7PZ9Lj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import yaml\n",
        "import gdown\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "\n",
        "def download_and_unzip(zip_url, output_folder='./Dataset',  folders_to_delete=None):\n",
        "    # Extract the Google Drive ID from the URL\n",
        "    file_id = zip_url.split('/')[-2]\n",
        "    download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "\n",
        "    # Check if download URL has public permissions set\n",
        "    try:\n",
        "        zip_file = \"downloaded_zip_file.zip\"\n",
        "        gdown.download(download_url, zip_file, quiet=False)\n",
        "\n",
        "        # Create output folder if it doesn't exist\n",
        "        if not os.path.exists(output_folder):\n",
        "            os.makedirs(output_folder)\n",
        "\n",
        "        # Unzip the downloaded file\n",
        "        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "            zip_ref.extractall(output_folder)\n",
        "\n",
        "        print(f\"Files extracted to '{output_folder}'\")\n",
        "\n",
        "        # Clean up the downloaded zip file\n",
        "        os.remove(zip_file)\n",
        "\n",
        "        # Clean up the Dataset folder\n",
        "        if folders_to_delete:\n",
        "            for folder in folders_to_delete:\n",
        "                folder_path = os.path.join(output_folder, folder)\n",
        "                if os.path.exists(folder_path) and os.path.isdir(folder_path):\n",
        "                    shutil.rmtree(folder_path)\n",
        "                    print(f\"Deleted folder: {folder_path}\")\n",
        "                else:\n",
        "                    print(f\"Folder not found or not a directory: {folder_path}\")\n",
        "\n",
        "        # Load the existing data.yaml file\n",
        "        data_yaml_path = './Dataset/data.yaml'\n",
        "        with open(data_yaml_path, 'r') as file:\n",
        "            data_config = yaml.safe_load(file)\n",
        "\n",
        "        # Update paths to be relative\n",
        "        data_config['train'] = './images/train'\n",
        "        data_config['val'] = './images/val'\n",
        "        data_config['test'] = './images/test'\n",
        "\n",
        "        # Write the updated content back to the data.yaml file with proper formatting for arrays\n",
        "        with open(data_yaml_path, 'w') as file:\n",
        "            yaml.dump(data_config, file, default_flow_style=None, sort_keys=False)\n",
        "\n",
        "        print(f\"Updated '{data_yaml_path}' with relative paths.\")\n",
        "\n",
        "    except gdown.exceptions.FileURLRetrievalError as e:\n",
        "        print(\"Failed to retrieve file url. Please check the permissions on Google Drive and try again.\")\n",
        "        print(e)\n",
        "\n",
        "# Example usage:\n",
        "zip_url = \"https://drive.google.com/file/d/1UknFvrQTOlIAxhkf3CHvbaSSsHru-fPP/view?usp=sharing\"\n",
        "download_and_unzip(zip_url, folders_to_delete=['.config', '.ipynb_checkpoints'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kd02LRjIo8DQ",
        "outputId": "2de7c787-e4b7-4557-8e17-917d57b03cbf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1UknFvrQTOlIAxhkf3CHvbaSSsHru-fPP\n",
            "From (redirected): https://drive.google.com/uc?id=1UknFvrQTOlIAxhkf3CHvbaSSsHru-fPP&confirm=t&uuid=4e8a0a81-9c5c-4688-8619-48e72c9627f3\n",
            "To: /content/downloaded_zip_file.zip\n",
            "100%|██████████| 154M/154M [00:03<00:00, 43.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files extracted to './Dataset'\n",
            "Deleted folder: ./Dataset/.config\n",
            "Deleted folder: ./Dataset/.ipynb_checkpoints\n",
            "Updated './Dataset/data.yaml' with relative paths.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 Evaluating baseline finetuned model"
      ],
      "metadata": {
        "id": "v2ikN2rCofxs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVHbAq3jnrMJ",
        "outputId": "1c39a9a7-1726-4648-c030-c7bc054bd95c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-06ef31402c3f>:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(\"resnet18_finetuned.pt\")\n",
            "100%|██████████| 47/47 [00:07<00:00,  6.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 99.80%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "from PIL import Image\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define a custom model class to include the forward method\n",
        "class FineTunedResNet18(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(FineTunedResNet18, self).__init__()\n",
        "        self.resnet18 = resnet18(weights = ResNet18_Weights.IMAGENET1K_V1)\n",
        "        # Modify the fully connected layer to match the number of classes\n",
        "        self.resnet18.fc = nn.Linear(self.resnet18.fc.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Define the forward pass\n",
        "        return self.resnet18(x)\n",
        "\n",
        "# Custom Dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, images_dir, labels_dir, transform=None):\n",
        "        self.images_dir = images_dir\n",
        "        self.labels_dir = labels_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # Get all image files in the images directory\n",
        "        self.image_files = sorted(os.listdir(images_dir))  # Sorting to align with labels\n",
        "        self.label_files = sorted(os.listdir(labels_dir))  # Sorting to align with images\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load image\n",
        "        img_path = os.path.join(self.images_dir, self.image_files[idx])\n",
        "        image = Image.open(img_path).convert('RGB')  # Convert to RGB if not already\n",
        "\n",
        "        # Load label\n",
        "        label_path = os.path.join(self.labels_dir, self.label_files[idx])\n",
        "        with open(label_path, 'r') as f:\n",
        "            label_idx = int(f.read().strip().split(\" \")[0])  # Read label from file\n",
        "\n",
        "        # Apply transformations if any\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label_idx\n",
        "\n",
        "# Load the fine-tuned model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = FineTunedResNet18(29)\n",
        "state_dict = torch.load(\"resnet18_finetuned.pt\")\n",
        "model.load_state_dict(state_dict)\n",
        "model = model.to(device)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "test_dataset = CustomDataset(images_dir='Dataset/images/test', labels_dir='Dataset/labels/test', transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in tqdm(test_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 AWQ"
      ],
      "metadata": {
        "id": "fmHE9npYoj2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AWQFineTunedResNet18(FineTunedResNet18):\n",
        "    def __init__(self, num_classes):\n",
        "        super(AWQFineTunedResNet18, self).__init__(num_classes)\n",
        "        self.quant = torch.quantization.QuantStub()\n",
        "        self.dequant = torch.quantization.DeQuantStub()\n",
        "        self.skip_add = nn.quantized.FloatFunctional()\n",
        "\n",
        "    def _make_layer_quantized(self, x, layer):\n",
        "        identity = x\n",
        "\n",
        "        # First conv block\n",
        "        out = layer.conv1(x)\n",
        "        out = layer.bn1(out)\n",
        "        out = layer.relu(out)\n",
        "\n",
        "        # Second conv block\n",
        "        out = layer.conv2(out)\n",
        "        out = layer.bn2(out)\n",
        "\n",
        "        # Handle downsample\n",
        "        if hasattr(layer, 'downsample') and layer.downsample is not None:\n",
        "            identity = layer.downsample(x)\n",
        "\n",
        "        # Quantized addition\n",
        "        out = self.skip_add.add(out, identity)\n",
        "        out = layer.relu(out)\n",
        "        return out\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initial quantization\n",
        "        x = self.quant(x)\n",
        "\n",
        "        # Initial conv + pooling\n",
        "        x = self.resnet18.conv1(x)\n",
        "        x = self.resnet18.bn1(x)\n",
        "        x = self.resnet18.relu(x)\n",
        "        x = self.resnet18.maxpool(x)\n",
        "\n",
        "        # Layer blocks with quantized residual connections\n",
        "        for block in self.resnet18.layer1:\n",
        "            x = self._make_layer_quantized(x, block)\n",
        "        for block in self.resnet18.layer2:\n",
        "            x = self._make_layer_quantized(x, block)\n",
        "        for block in self.resnet18.layer3:\n",
        "            x = self._make_layer_quantized(x, block)\n",
        "        for block in self.resnet18.layer4:\n",
        "            x = self._make_layer_quantized(x, block)\n",
        "\n",
        "        x = self.resnet18.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.resnet18.fc(x)\n",
        "\n",
        "        # Final dequantization\n",
        "        x = self.dequant(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "ADB0hQ8yP40b"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_model_for_awq(model, calibration_loader, alpha=0.2, per_channel_scaling_factor=1.5):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # 1. Capture Activations Using Hooks\n",
        "    activations = {}\n",
        "    def save_activation(name):\n",
        "        def hook(module, input, output):\n",
        "            activations[name] = output.detach()\n",
        "        return hook\n",
        "\n",
        "    for name, layer in model.resnet18.named_modules():\n",
        "        if isinstance(layer, nn.Conv2d):\n",
        "            layer.register_forward_hook(save_activation(name))\n",
        "\n",
        "    # 2. Forward pass\n",
        "    calibration_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in calibration_loader:\n",
        "            images = images.to(device)\n",
        "            _ = model(images)\n",
        "            break\n",
        "\n",
        "    # 3. Analyze Activation Distributions\n",
        "    activation_stats = {}\n",
        "\n",
        "    for name, act in activations.items():\n",
        "        B, C, H, W = act.shape\n",
        "        per_channel_abs_mean = act.abs().mean(dim=[0,2,3])\n",
        "        activation_stats[name] = per_channel_abs_mean\n",
        "\n",
        "    # 4. Compute Per-Channel Scaling\n",
        "    for name, layer in model.resnet18.named_modules():\n",
        "        if isinstance(layer, nn.Conv2d) and name in activation_stats:\n",
        "            per_channel_abs_mean = activation_stats[name]\n",
        "            s_c = per_channel_abs_mean**alpha\n",
        "            s_c = s_c.clamp(min=1e-8)\n",
        "\n",
        "            s_c_reshaped = s_c.view(-1,1,1,1)\n",
        "            weights = layer.weight.data\n",
        "            layer.weight.data *= s_c_reshaped\n",
        "\n",
        "    # 5. Identify Outliers Per Channel\n",
        "    outliers = {}\n",
        "\n",
        "    for name, layer in model.resnet18.named_modules():\n",
        "        if isinstance(layer, nn.Conv2d):\n",
        "            weights = layer.weight.data\n",
        "            outlier_threshold = weights.abs().quantile(0.99).item()\n",
        "\n",
        "            outliers_mask = weights.abs() > outlier_threshold\n",
        "            outliers[name] = outliers_mask\n",
        "\n",
        "    # 6. Quantize Non-Outliers\n",
        "    per_channel_scaling_factor = 1.5\n",
        "\n",
        "    for name, layer in model.resnet18.named_modules():\n",
        "        if name in outliers:\n",
        "            weights = layer.weight.data\n",
        "\n",
        "            # Extra scaling for salient weights\n",
        "            outlier_weights = weights[outliers[name]].float()\n",
        "            outlier_weights *= per_channel_scaling_factor\n",
        "            weights[outliers[name]] = outlier_weights\n",
        "\n",
        "            non_outliers = ~outliers[name]\n",
        "            non_outlier_weights = weights[non_outliers].float()\n",
        "\n",
        "            # Compute scale for quantization\n",
        "            w_min = non_outlier_weights.min().item()\n",
        "            w_max = non_outlier_weights.max().item()\n",
        "            scale = (w_max - w_min) / 255.0 if (w_max > w_min) else 1e-4\n",
        "\n",
        "            # Quantize\n",
        "            quantized_weights = torch.quantize_per_tensor(\n",
        "                non_outlier_weights, scale=scale, zero_point=0, dtype=torch.qint8\n",
        "            ).dequantize()\n",
        "\n",
        "            # weights[non_outliers] = quantized_weights.int_repr().to(dtype=torch.float)\n",
        "            weights[non_outliers] = quantized_weights\n",
        "\n",
        "    # Proceed with Quantization\n",
        "    model = model.to(\"cpu\")\n",
        "    model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
        "    torch.backends.quantized.engine = 'fbgemm'\n",
        "\n",
        "    torch.quantization.fuse_modules(\n",
        "        model.resnet18,\n",
        "        [['conv1', 'bn1', 'relu']],\n",
        "        inplace=True\n",
        "    )\n",
        "\n",
        "    torch.quantization.prepare(model, inplace=True)\n",
        "\n",
        "    # Calibration with data\n",
        "    with torch.no_grad():\n",
        "        for i, (data, _) in enumerate(calibration_loader):\n",
        "            model(data)\n",
        "            if i >= 100:  # Use more calibration batches\n",
        "                break\n",
        "\n",
        "    torch.quantization.convert(model, inplace=True)\n",
        "    return model"
      ],
      "metadata": {
        "id": "GxHOl56BOIBu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device('cpu')\n",
        "\n",
        "alphas = np.linspace(0.1, 0.3, 5)\n",
        "salient_scales = np.linspace(1.25, 2, 4)\n",
        "accuracies = {}\n",
        "\n",
        "best_model = AWQFineTunedResNet18(num_classes=29)\n",
        "best_accuracy = 0.0\n",
        "\n",
        "for salient_scale in salient_scales:\n",
        "    for alpha in alphas:\n",
        "        print(f'Alpha: {alpha}, Salient Scale: {salient_scale}')\n",
        "\n",
        "        model = AWQFineTunedResNet18(num_classes=29)\n",
        "\n",
        "        # Load pre-trained weights\n",
        "        state_dict = torch.load('resnet18_finetuned.pt', map_location=device)\n",
        "        model.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "        # Data transformation\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "\n",
        "        # Create calibration dataset and loader\n",
        "        cal_dataset = CustomDataset(\n",
        "            images_dir='Dataset/images/val',\n",
        "            labels_dir='Dataset/labels/val',\n",
        "            transform=transform\n",
        "        )\n",
        "        calibration_loader = DataLoader(\n",
        "            cal_dataset,\n",
        "            batch_size=32,\n",
        "            shuffle=False\n",
        "        )\n",
        "\n",
        "        # Apply AWQ quantization preprocessing\n",
        "        model = prepare_model_for_awq(model, calibration_loader, alpha=alpha,\n",
        "                                      per_channel_scaling_factor=salient_scale)\n",
        "\n",
        "        # Evaluate the model\n",
        "        test_dataset = CustomDataset(\n",
        "            images_dir='Dataset/images/test',\n",
        "            labels_dir='Dataset/labels/test',\n",
        "            transform=transform\n",
        "        )\n",
        "        test_loader = DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size=32,\n",
        "            shuffle=False\n",
        "        )\n",
        "\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for data, labels in test_loader:\n",
        "                data, labels = data.to(device), labels.to(device)\n",
        "                outputs = model(data)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        accuracy = 100 * correct/total\n",
        "        print(f'Accuracy on test set: {accuracy}')\n",
        "        if (accuracy > best_accuracy):\n",
        "            best_model = model\n",
        "            best_accuracy = accuracy\n",
        "\n",
        "        accuracies[(alpha, salient_scale)] = accuracy\n",
        "\n",
        "\n",
        "# Save the quantized model\n",
        "torch.save({\n",
        "    'model_state_dict': best_model.state_dict(),\n",
        "    'accuracy': 100 * correct / total,\n",
        "}, 'resnet18_best_awq_quantized.pt')\n",
        "print(\"Best Model saved as resnet18_best_awq_quantized.pt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbdvIrGmPxi-",
        "outputId": "a4af1991-5539-4dd8-9877-79cfc9c4e4c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alpha: 0.1, Salient Scale: 1.25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-8361bb2d7ac6>:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load('resnet18_finetuned.pt', map_location=device)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test set: 68.6\n",
            "Alpha: 0.15, Salient Scale: 1.25\n",
            "Accuracy on test set: 73.6\n",
            "Alpha: 0.2, Salient Scale: 1.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib as plt\n",
        "\n"
      ],
      "metadata": {
        "id": "dgZ-EkeRjwL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Specify the path to your file\n",
        "file_path = 'resnet18_best_awq_quantized.pt'\n",
        "\n",
        "# Trigger the download\n",
        "files.download(file_path)\n"
      ],
      "metadata": {
        "id": "8vriY21amZEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "alphas = np.linspace(0.05, 0.4, 8)\n",
        "for alpha in alphas:\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = FineTunedResNet18(29)\n",
        "    state_dict = torch.load(\"resnet18_finetuned.pt\", map_location=device)\n",
        "    model.load_state_dict(state_dict)\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # 1. Capture Activations Using Hooks\n",
        "    activations = {}\n",
        "\n",
        "    def save_activation(name):\n",
        "        def hook(module, input, output):\n",
        "            activations[name] = output.detach()\n",
        "        return hook\n",
        "\n",
        "    for name, layer in model.resnet18.named_modules():\n",
        "        if isinstance(layer, nn.Conv2d):\n",
        "            layer.register_forward_hook(save_activation(name))\n",
        "\n",
        "    # 2. Forward pass\n",
        "    calibration_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in calibration_loader:\n",
        "            images = images.to(device)\n",
        "            _ = model(images)\n",
        "            break\n",
        "\n",
        "    # 3. Analyze Activation Distributions\n",
        "    activation_stats = {}\n",
        "\n",
        "    for name, act in activations.items():\n",
        "        B, C, H, W = act.shape\n",
        "        # act shape: [B, C, H, W]\n",
        "        # per-channel mean of absolute values\n",
        "        per_channel_abs_mean = act.abs().mean(dim=[0,2,3])\n",
        "        activation_stats[name] = per_channel_abs_mean\n",
        "\n",
        "    # 4. Compute Per-Channel Scaling\n",
        "    # alpha = 0.15\n",
        "    for name, layer in model.resnet18.named_modules():\n",
        "        if isinstance(layer, nn.Conv2d) and name in activation_stats:\n",
        "            per_channel_abs_mean = activation_stats[name]\n",
        "            s_c = per_channel_abs_mean**alpha\n",
        "            s_c = s_c.clamp(min=1e-8)\n",
        "\n",
        "            # layer.weight: [out_channels, in_channels, kH, kW]\n",
        "            # act shape: [batch size, channels, H, W]\n",
        "            s_c_reshaped = s_c.view(-1,1,1,1) # [C, 1, 1, 1]\n",
        "            weights = layer.weight.data\n",
        "            layer.weight.data *= s_c_reshaped\n",
        "\n",
        "    # 5. Identify Outliers Per Channel\n",
        "    outliers = {}\n",
        "\n",
        "    for name, layer in model.resnet18.named_modules():\n",
        "        if isinstance(layer, nn.Conv2d):\n",
        "            weights = layer.weight.data\n",
        "            outlier_threshold = weights.abs().quantile(0.99).item()\n",
        "\n",
        "            outliers_mask = weights.abs() > outlier_threshold\n",
        "            outliers[name] = outliers_mask\n",
        "\n",
        "    # 6. Quantize Non-Outliers\n",
        "    per_channel_scaling_factor = 1.5\n",
        "\n",
        "    for name, layer in model.resnet18.named_modules():\n",
        "        if name in outliers:\n",
        "            weights = layer.weight.data\n",
        "\n",
        "            # Extra scaling for salient weights\n",
        "            outlier_weights = weights[outliers[name]].float()\n",
        "            outlier_weights *= per_channel_scaling_factor\n",
        "            weights[outliers[name]] = outlier_weights\n",
        "\n",
        "            non_outliers = ~outliers[name]\n",
        "            non_outlier_weights = weights[non_outliers].float()\n",
        "\n",
        "            # Compute scale for quantization\n",
        "            w_min = non_outlier_weights.min().item()\n",
        "            w_max = non_outlier_weights.max().item()\n",
        "            scale = (w_max - w_min) / 255.0 if (w_max > w_min) else 1e-4\n",
        "\n",
        "            # Quantize\n",
        "            quantized_weights = torch.quantize_per_tensor(\n",
        "                non_outlier_weights, scale=scale, zero_point=0, dtype=torch.qint8\n",
        "            ).dequantize()\n",
        "\n",
        "            # weights[non_outliers] = quantized_weights.int_repr().to(dtype=torch.float)\n",
        "            weights[non_outliers] = quantized_weights\n",
        "\n",
        "            # layer.weight.data = layer.weight.data.to(dtype=torch.int8)\n",
        "\n",
        "    # Evaluate the model\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(test_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# 7. Save the Quantized Model\n",
        "torch.save(model.state_dict(), \"resnet18_awq_quantized.pt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82GwFWgQ4Cjl",
        "outputId": "1e4bd828-39e1-4d7a-8b40-1f0e79ae5d7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-cdf68e301bec>:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(\"resnet18_finetuned.pt\", map_location=device)\n",
            "100%|██████████| 47/47 [00:07<00:00,  6.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 67.80%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 47/47 [00:07<00:00,  6.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 76.60%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 47/47 [00:06<00:00,  6.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 78.73%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 47/47 [00:08<00:00,  5.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 79.40%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 47/47 [00:07<00:00,  6.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 65.40%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 47/47 [00:06<00:00,  6.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 47.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 47/47 [00:07<00:00,  6.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 30.80%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 47/47 [00:06<00:00,  6.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 24.53%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in tqdm(test_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giIeqjsMnkUl",
        "outputId": "aa531afd-8699-4d5f-fdcd-a148a60c60bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 47/47 [00:06<00:00,  6.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 78.73%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the quantized model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = FineTunedResNet18(29)\n",
        "state_dict = torch.load(\"resnet18_awq_quantized.pt\")\n",
        "model.load_state_dict(state_dict)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "35USKsoD9UEv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b299a77b-ed3f-4ca2-805a-7a2e40141097"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-38-1138a68beb04>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(\"resnet18_awq_quantized.pt\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the fine-tuned model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = FineTunedResNet18(29)\n",
        "state_dict = torch.load(\"resnet18_finetuned.pt\")\n",
        "model.load_state_dict(state_dict)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmthbHBWOupW",
        "outputId": "052d4845-2672-4d22-e786-d7137653af7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-40-f78c020d20cd>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(\"resnet18_finetuned.pt\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SmoothQuantFineTunedResNet18(FineTunedResNet18):\n",
        "    def __init__(self, num_classes):\n",
        "        super(SmoothQuantFineTunedResNet18, self).__init__(num_classes)\n",
        "        self.quant = torch.quantization.QuantStub()\n",
        "        self.dequant = torch.quantization.DeQuantStub()\n",
        "        self.skip_add = nn.quantized.FloatFunctional()\n",
        "\n",
        "    def _make_layer_quantized(self, x, layer):\n",
        "        identity = x\n",
        "\n",
        "        # First conv block\n",
        "        out = layer.conv1(x)\n",
        "        out = layer.bn1(out)\n",
        "        out = layer.relu(out)\n",
        "\n",
        "        # Second conv block\n",
        "        out = layer.conv2(out)\n",
        "        out = layer.bn2(out)\n",
        "\n",
        "        # Handle downsample\n",
        "        if hasattr(layer, 'downsample') and layer.downsample is not None:\n",
        "            identity = layer.downsample(x)\n",
        "\n",
        "        # Quantized addition\n",
        "        out = self.skip_add.add(out, identity)\n",
        "        out = layer.relu(out)\n",
        "        return out\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initial quantization\n",
        "        x = self.quant(x)\n",
        "\n",
        "        # Initial conv + pooling\n",
        "        x = self.resnet18.conv1(x)\n",
        "        x = self.resnet18.bn1(x)\n",
        "        x = self.resnet18.relu(x)\n",
        "        x = self.resnet18.maxpool(x)\n",
        "\n",
        "        # Layer blocks with quantized residual connections\n",
        "        for block in self.resnet18.layer1:\n",
        "            x = self._make_layer_quantized(x, block)\n",
        "        for block in self.resnet18.layer2:\n",
        "            x = self._make_layer_quantized(x, block)\n",
        "        for block in self.resnet18.layer3:\n",
        "            x = self._make_layer_quantized(x, block)\n",
        "        for block in self.resnet18.layer4:\n",
        "            x = self._make_layer_quantized(x, block)\n",
        "\n",
        "        x = self.resnet18.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.resnet18.fc(x)\n",
        "\n",
        "        # Final dequantization\n",
        "        x = self.dequant(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "8YahHdrrPnHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load original finetuned model\n",
        "finetuned_model = FineTunedResNet18(num_classes=29)\n",
        "finetuned_state = torch.load('resnet18_finetuned.pt')\n",
        "finetuned_model.load_state_dict(finetuned_state)\n",
        "\n",
        "# Load quantized model\n",
        "quantized_model = FineTunedResNet18(num_classes=29)\n",
        "quantized_state = torch.load('resnet18_awq_quantized.pt')\n",
        "quantized_model.load_state_dict(quantized_state)\n",
        "\n",
        "# Compare weights and print statistics\n",
        "print(\"Comparing models:\")\n",
        "for (name1, param1), (name2, param2) in zip(finetuned_model.named_parameters(),\n",
        "                                            quantized_model.named_parameters()):\n",
        "    if \"conv\" in name1 and param1.data.shape == param2.data.shape:\n",
        "        diff = torch.abs(param1.data - param2.data)\n",
        "        print(f\"\\nLayer: {name1}\")\n",
        "        print(f\"Max difference: {torch.max(diff).item():.6f}\")\n",
        "        print(f\"Mean difference: {torch.mean(diff).item():.6f}\")\n",
        "        print(f\"Data type: {param1.data.dtype} vs {param2.data.dtype}\")\n",
        "        print(f\"Memory size: {param1.data.element_size() * param1.data.nelement()} vs \"\n",
        "              f\"{param2.data.element_size() * param2.data.nelement()} bytes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYJeEBR1Oj9s",
        "outputId": "e7cca39c-f5c9-455f-fe19-6a5af4b1fcc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-52-dfab4c4b25c9>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  finetuned_state = torch.load('resnet18_finetuned.pt')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing models:\n",
            "\n",
            "Layer: resnet18.conv1.weight\n",
            "Max difference: 126.659073\n",
            "Mean difference: 19.414095\n",
            "Data type: torch.float32 vs torch.float32\n",
            "Memory size: 37632 vs 37632 bytes\n",
            "\n",
            "Layer: resnet18.layer1.0.conv1.weight\n",
            "Max difference: 126.848892\n",
            "Mean difference: 16.712107\n",
            "Data type: torch.float32 vs torch.float32\n",
            "Memory size: 147456 vs 147456 bytes\n",
            "\n",
            "Layer: resnet18.layer1.0.conv2.weight\n",
            "Max difference: 127.765190\n",
            "Mean difference: 23.079416\n",
            "Data type: torch.float32 vs torch.float32\n",
            "Memory size: 147456 vs 147456 bytes\n",
            "\n",
            "Layer: resnet18.layer1.1.conv1.weight\n",
            "Max difference: 126.869019\n",
            "Mean difference: 21.437405\n",
            "Data type: torch.float32 vs torch.float32\n",
            "Memory size: 147456 vs 147456 bytes\n",
            "\n",
            "Layer: resnet18.layer1.1.conv2.weight\n",
            "Max difference: 126.912766\n",
            "Mean difference: 24.642159\n",
            "Data type: torch.float32 vs torch.float32\n",
            "Memory size: 147456 vs 147456 bytes\n",
            "\n",
            "Layer: resnet18.layer2.0.conv1.weight\n",
            "Max difference: 127.861313\n",
            "Mean difference: 24.389071\n",
            "Data type: torch.float32 vs torch.float32\n",
            "Memory size: 294912 vs 294912 bytes\n",
            "\n",
            "Layer: resnet18.layer2.0.conv2.weight\n",
            "Max difference: 126.934349\n",
            "Mean difference: 22.651003\n",
            "Data type: torch.float32 vs torch.float32\n",
            "Memory size: 589824 vs 589824 bytes\n",
            "\n",
            "Layer: resnet18.layer2.1.conv1.weight\n",
            "Max difference: 127.886162\n",
            "Mean difference: 24.348106\n",
            "Data type: torch.float32 vs torch.float32\n",
            "Memory size: 589824 vs 589824 bytes\n",
            "\n",
            "Layer: resnet18.layer2.1.conv2.weight\n",
            "Max difference: 126.944603\n",
            "Mean difference: 25.214821\n",
            "Data type: torch.float32 vs torch.float32\n",
            "Memory size: 589824 vs 589824 bytes\n",
            "\n",
            "Layer: resnet18.layer3.0.conv1.weight\n",
            "Max difference: 126.941429\n",
            "Mean difference: 25.361765\n",
            "Data type: torch.float32 vs torch.float32\n",
            "Memory size: 1179648 vs 1179648 bytes\n",
            "\n",
            "Layer: resnet18.layer3.0.conv2.weight\n",
            "Max difference: 127.915512\n",
            "Mean difference: 27.070652\n",
            "Data type: torch.float32 vs torch.float32\n",
            "Memory size: 2359296 vs 2359296 bytes\n",
            "\n",
            "Layer: resnet18.layer3.1.conv1.weight\n",
            "Max difference: 127.950600\n",
            "Mean difference: 27.581511\n",
            "Data type: torch.float32 vs torch.float32\n",
            "Memory size: 2359296 vs 2359296 bytes\n",
            "\n",
            "Layer: resnet18.layer3.1.conv2.weight\n",
            "Max difference: 126.962769\n",
            "Mean difference: 25.330387\n",
            "Data type: torch.float32 vs torch.float32\n",
            "Memory size: 2359296 vs 2359296 bytes\n",
            "\n",
            "Layer: resnet18.layer4.0.conv1.weight\n",
            "Max difference: 127.957115\n",
            "Mean difference: 29.851572\n",
            "Data type: torch.float32 vs torch.float32\n",
            "Memory size: 4718592 vs 4718592 bytes\n",
            "\n",
            "Layer: resnet18.layer4.0.conv2.weight\n",
            "Max difference: 127.948418\n",
            "Mean difference: 30.768633\n",
            "Data type: torch.float32 vs torch.float32\n",
            "Memory size: 9437184 vs 9437184 bytes\n",
            "\n",
            "Layer: resnet18.layer4.1.conv1.weight\n",
            "Max difference: 127.955978\n",
            "Mean difference: 33.732285\n",
            "Data type: torch.float32 vs torch.float32\n",
            "Memory size: 9437184 vs 9437184 bytes\n",
            "\n",
            "Layer: resnet18.layer4.1.conv2.weight\n",
            "Max difference: 126.977196\n",
            "Mean difference: 30.949760\n",
            "Data type: torch.float32 vs torch.float32\n",
            "Memory size: 9437184 vs 9437184 bytes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-52-dfab4c4b25c9>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  quantized_state = torch.load('resnet18_awq_quantized.pt')\n"
          ]
        }
      ]
    }
  ]
}